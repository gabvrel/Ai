# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Zdu3EA9wUD0t7Q9oF0pEhwOc_ZFPrza

# Python Implementation
"""

import numpy as np
import scipy as sc
import time
from IPython.display import clear_output

data = np.array([[255, 0, 0, 0],
                 [0, 255, 0, 1],
                 [0, 0, 255, 0],
                 [255, 255, 0, 1],
                 [255, 0, 255, 0],
                 [0, 255, 255, 1],
                 [0, 0, 0, 0],
                 [255, 255, 255, 1],
                 [163, 3, 14, 0],
                 [20, 26, 254, 0]
                 ])

data = np.hsplit(data, np.array([3, 6]))

X = data[0]
Y = data[1]
p = 3
print(Y)


class neural_layer():
    def __init__(self, n_conn, n_neur, act_f):
        self.act_f = act_f
        self.b = np.random.rand(1, n_neur) * 2 - 1
        self.w = np.random.rand(n_conn, n_neur) * 2 - 1


sigm = (lambda x: 1 / (1 + np.e ** (-x)),
        lambda x: x * (1 - x))

relu = lambda x: np.maximum(0, x)


def create_nn(topology, act_f):
    nn = []

    for l, layer in enumerate(topology[:-1]):
        nn.append(neural_layer(topology[l], topology[l + 1], act_f))

    return nn


topology = [p, 6, 12, 6, 1]

neural_net = create_nn(topology, sigm)

l2_cost = (lambda Yp, Yr: np.mean((Yp - Yr) ** 2),
           lambda Yp, Yr: (Yp - Yr))


def train(neural_net, X, Y, l2_cost, lr=0.5, train=True):
    out = [(None, X)]

    for l, layer in enumerate(neural_net):
        z = out[-1][1] @ neural_net[l].w + neural_net[l].b  #
        a = neural_net[l].act_f[0](z)
        out.append((z, a))

    if train:

        deltas = []

        for l in reversed(range(0, len(neural_net))):

            z = out[l + 1][0]
            a = out[l + 1][1]

            if l == len(neural_net) - 1:
                deltas.insert(0, l2_cost[1](a, Y) * neural_net[l].act_f[1](a))
            else:
                deltas.insert(0, deltas[0] @ _W.T * neural_net[l].act_f[1](a))

            _W = neural_net[l].w

            neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, keepdims=True) * lr
            neural_net[l].w = neural_net[l].w - out[l][1].T @ deltas[0] * lr

    return out[-1][1]


loss = []
test_arr = [[0, 0, 10]]

for i in range(5000):
    pY = train(neural_net, X, Y, l2_cost, lr=0.055)

    if i % 250 == 0:
        loss.append(l2_cost[0](pY, Y))

        clear_output(wait=True)
        plt.plot(range(len(loss)), loss)
        plt.show()
        time.sleep(0.5)

test_arr = [[240, 255, 0]]
print(train(neural_net, test_arr, Y, l2_cost, lr=0.055, train=False))

"""# TensorFlow Adaptation"""

import matplotlib.pyplot as plt
import tensorflow as tf
import logging

logger = tf.get_logger()
logger.setLevel(logging.ERROR)

l0 = tf.keras.layers.Dense(units=3, input_shape=[3])
l1 = tf.keras.layers.Dense(units=6)
l2 = tf.keras.layers.Dense(units=12)
l3 = tf.keras.layers.Dense(units=6)
l4 = tf.keras.layers.Dense(units=1)

model = tf.keras.Sequential([l0, l1, l2, l3, l4])

model.compile(loss="mean_squared_error", optimizer=tf.keras.optimizers.Adam(0.01))
history = model.fit(X, Y, epochs=300, verbose=False)

test_arr = [[255, 240, 220]]
test_arr = np.array(test_arr)

plt.xlabel('Epoch Number')
plt.ylabel("Loss Magnitude")
plt.plot(history.history['loss'])
plt.show()

print(model.predict(test_arr))